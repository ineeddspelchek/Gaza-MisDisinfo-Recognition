{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ineeddspelchek/Gaza-MisDisinfo-Recognition/blob/main/CS_4710_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install PyICU"
      ],
      "metadata": {
        "id": "QPlvZ78K5diw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e3c3d8-fc8b-4a55-f2a9-b5ca6213109c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyICU\n",
            "  Downloading PyICU-2.13.1.tar.gz (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: PyICU\n",
            "  Building wheel for PyICU (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.13.1-cp310-cp310-linux_x86_64.whl size=1801404 sha256=058f7137ba03fb965745fbc71ee02a4be5a1058b2442ebdcecef12ba465fab45\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/45/08/f4bc505e31eb7bb779d493226921abf18cf55ed30142e70eae\n",
            "Successfully built PyICU\n",
            "Installing collected packages: PyICU\n",
            "Successfully installed PyICU-2.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "_o9MQhzq4njD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oMCtAHS9-GP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5725f60b-2925-49e9-be92-e96611b44b4f"
      },
      "outputs": [
      ],
      "source": [
        "# Preprocessing/Data - Henry\n",
        "'''\n",
        "Sources:\n",
        "https://stackoverflow.com/questions/48376580/how-to-read-data-in-google-colab-from-my-google-drive\n",
        "https://stackoverflow.com/questions/62637553/how-to-get-all-unicode-characters-of-a-language-by-using-its-iso-language-code-i\n",
        "https://www.freecodecamp.org/news/how-to-remove-a-specific-character-from-a-string-in-python/\n",
        "https://www.w3schools.com/python/\n",
        "https://stackoverflow.com/questions/33803306/split-strings-on-whitespaces-but-do-not-remove-them\n",
        "https://www.rexegg.com/regex-quickstart.html\n",
        "https://stackoverflow.com/questions/12453580/how-to-concatenate-join-items-in-a-list-to-a-single-string\n",
        "https://skills.ai/blog/import-google-sheets-to-pandas/\n",
        "https://stackoverflow.com/questions/399078/what-special-characters-must-be-escaped-in-regular-expressions\n",
        "https://stackoverflow.com/questions/4172448/is-it-possible-to-break-a-long-line-to-multiple-lines-in-python\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import icu\n",
        "import re\n",
        "\n",
        "googleSheetURL = \"https://docs.google.com/spreadsheets/d/10gKGxnbHhRqv0PgDucE36ks93yjSHLgN/export?format=xlsx\"\n",
        "\n",
        "IMPORTED_DATA = pd.read_excel(googleSheetURL, sheet_name=\"Fact Checks\", header=None).to_numpy().tolist()\n",
        "np.random.shuffle(IMPORTED_DATA)\n",
        "\n",
        "HEBREW_CHARACTERS = icu.LocaleData('he').getExemplarSet(0, 0)\n",
        "ARABIC_CHARACTERS = icu.LocaleData('ar').getExemplarSet(0, 0)\n",
        "\n",
        "ISRAEL_IDENTIFIERS = pd.read_excel(googleSheetURL, sheet_name=\"Swap Chart\", header=None).to_numpy().transpose()[0].tolist()\n",
        "PALESTINE_IDENTIFIERS = pd.read_excel(googleSheetURL, sheet_name=\"Swap Chart\", header=None).to_numpy().transpose()[1].tolist()\n",
        "\n",
        "SPLIT_DELIMS = \"([\" + \\\n",
        "\"\\\"\\-#\\s.,;()!?\" + \\\n",
        "\"😀😃😄😁😆🥹😅😂🤣😍😌😉🙃🙂😇😊☺️🥲🥰😘😗😙😚😋😛😝😜😏🥳🤩🥸😎🤓🧐🤨🤪😒😞😔😟😕🙁☹️😣😖🤬😡😠😤😭😢🥺😩😫🤯😳🥵🥶😶‍🌫️😱😨😰😥🫠🤫🫡🫢🤭🫣🤔🤗😓🤥😶🫥😐🫤😑😬🙄😯😮‍💨😪🤤😴🥱😲😮😧😦😵😵‍💫🤐🥴🤢🤮🤧😷🤒💩🤡👺👹👿😈🤠🤑🤕👻💀☠️👽👾🤖🎃😺😸🤲🫶😾😿🙀😽😼😻😹👐🙌👏🤝👍👎✊🤛👊🤏🤌👌🤘🤟🫰✌️🤞🤜🫳🫴👈👉👆👇☝️✋🤚🖕🦾💪🫱🫲🤙👋🖖🖐✍️🙏🫵🦶🦵🦿💄💋👄👀👁👣👃🦻👂👅🦷🫦🫀🫁🧠🗣👤👥🫂👶👧👩‍🦰👨‍🦱🧑‍🦱👩‍🦱👨🧑👩👦🧒🧑‍🦰👨‍🦰👱‍♀️👱👱‍♂️👩‍🦳🧑‍🦳👨‍🦳👩‍🦲👲👴🧓👵🧔‍♂️🧔🧔‍♀️👨‍🦲🧑‍🦲👳‍♀️👳👳‍♂️🧕👮‍♀️👮👮‍♂️👷‍♀️👷🧑‍⚕️👩‍⚕️🕵️‍♂️🕵️🕵️‍♀️💂‍♂️💂💂‍♀️👷‍♂️👨‍⚕️👩‍🌾🧑‍🌾👩‍🍳🧑‍🍳👨‍🍳👩‍🎓🧑‍🎓👨‍🌾🧑‍🏭👩‍🏭👨‍🏫🧑‍🏫👩‍🏫👨‍🎤🧑‍🎤👩‍🎤👨‍🎓👨‍🏭👩‍💻🧑‍💻👨‍💻👩‍💼🧑‍💼👨‍💼👩‍🔧🧑‍🔧🧑‍🚒👩‍🚒👨‍🎨🧑‍🎨👩‍🎨👨‍🔬🧑‍🔬👩‍🔬👨‍🔧👨‍🚒👩‍✈️🧑‍✈️👨‍✈️👩‍🚀🧑‍🚀👨‍🚀👩‍⚖️🧑‍⚖️🫅👸🤵‍♂️🤵🤵‍♀️👰‍♂️👰👰‍♀️👨‍⚖️🤴🥷🦸‍♀️🦸🦸‍♂️🦹🦹‍♂️🤶🦹‍♀️🧌🧝‍♂️🧝🧝‍♀️🧙‍♂️🧙🧙‍♀️🎅🧑‍🎄🧛‍♀️🧛🧛‍♂️🧟‍♀️🧟🧟‍♂️🧞‍♀️🧞🧞‍♂️🫄🤰👼🧚‍♂️🧚🧚‍♀️🧜‍♂️🧜🧜‍♀️🫃🤱👩‍🍼🧑‍🍼👨‍🍼🙇‍♀️🙇🙇‍♂️💁‍♀️🙋‍♀️🙆‍♂️🙆🙆‍♀️🙅‍♂️🙅🙅‍♀️💁‍♂️💁🙋🙋‍♂️🧏‍♀️🧏🧏‍♂️🤦‍♀️🤦🤦‍♂️🤷‍♀️💇‍♀️🙍‍♂️🙍🙍‍♀️🙎‍♂️🙎🙎‍♀️🤷‍♂️🤷💇💇‍♂️💆‍♀️💆💆‍♂️🧖‍♀️🧖🧖‍♂️💅🧑‍🦽👩‍🦽🕴👯‍♂️👯👯‍♀️🕺💃🤳👨‍🦽🧑‍🦼👨‍🦼🚶‍♀️🚶🚶‍♂️👩‍🦯🧑‍🦯🧍🧍‍♀️🏃‍♂️🏃🏃‍♀️🧎‍♂️🧎🧎‍♀️👨‍🦯🧍‍♂️👫👭👬👩‍❤️‍👨👩‍❤️‍👩💑👨‍❤️‍👨👩‍❤️‍💋‍👨👩‍👩‍👦👨‍👩‍👧‍👧👨‍👩‍👦‍👦👨‍👩‍👧‍👦👨‍👩‍👧👨‍👩‍👦👨‍❤️‍💋‍👨💏👩‍❤️‍💋‍👩👩‍👩‍👧👩‍👩‍👧‍👦👩‍👩‍👦‍👦👩‍👩‍👧‍👧👨‍👨‍👦👨‍👨‍👧👨‍👨‍👧‍👦👨‍👨‍👦‍👦👨‍👨‍👧‍👧👨‍👦‍👦👨‍👧‍👦👨‍👧👨‍👦👩‍👧‍👧👩‍👦‍👦👩‍👧‍👦👩‍👧👩‍👦👨‍👧‍👧🪢🧶🧵🪡🧥🥼🦺👚👘🩱👙👗👔🩳🩲👖👕🥻🩴🥿👠👡👢👞👟🥾🪖⛑👒🎓🧢🎩🧣🧤🧦👑💍👝👛👜💼🎒🧳👓🌂🥽🕶❤️🧡💛💚💙💜🤎🖤🤍💖💗💓💞💕❣️❤️‍🩹❤️‍🔥💔💘💝♡♥︎❥❦❧☙❢এლდღɞʚෆᰔᩚᰔঞଓᜊᥫ᭡ꨄஐᦗ🐶🐱🐭🐹🐰🦊🐻🐼🐻‍❄️🙈🐵🐸🐽🐷🐮🦁🐯🐨🙉🙊🐒🐔🐧🐦🐤🐣🐥🐝🦄🐴🐗🐺🦇🦉🦅🦆🪱🐛🦋🐌🐞🐜🪰🪲🪳🦖🦎🐍🐢🦂🕸🕷🦗🦟🦕🐙🦑🦐🦞🦀🐡🐠🐟🦓🐆🐅🐊🦭🦈🐋🐳🐬🦍🦧🦣🐘🦛🦏🐪🐫🦒🐑🐏🐖🐎🐄🐂🐃🦬🦘🦙🐐🦌🐕🐩🦮🐕‍🦺🐈🐈‍⬛🕊🦩🦢🦜🦚🦤🦃🐓🪶🐇🦝🦨🦡🦫🦦🦥🐁🐀🌳🌲🎄🌵🐲🐉🐾🦔🐿🌴🪵🌱🌿☘️🍀🎍🪴🎋🪨🪸🐚🍄🪹🪺🍁🍂🍃🌾💐🌷🌹🥀🪷🌺🌸🌼🌗🌖🌕🌚🌜🌛🌝🌞🌻🌘🌑🌒🌓🌔🌙🌎🌍🌏🔥💥☄️⚡️✨🌟⭐️💫🪐🌪🌈☀️🌤⛅️🌥☁️🌦🌧💧💨🌬⛄️☃️❄️🌨🌩⛈💦🫧☔️☂️🌊🍏🍎🍐🍊🍋🍌🍉🍇🍓🍅🥝🥥🍍🥭🍑🍒🍈🫐🍆🥑🥦🥬🥒🌶🫑🌽🥕🥖🍞🥯🥐🍠🥔🧅🧄🫒🥨🧀🥚🍳🧈🥞🧇🥓🥩🥪🫓🍕🍟🍔🌭🦴🍖🍗🥙🧆🌮🌯🫔🥗🥘🫕🥫🦪🥟🍱🍣🍛🍲🍜🍝🫙🍤🍙🍚🍘🍥🥠🥮🍢🍡🍭🍮🎂🍰🧁🥧🍦🍨🍧🍬🍫🍿🍩🍪🌰🥜🫘🍯🧋🥤🧃🍵☕️🫖🍼🫗🥛🍶🍺🍻🥂🍷🥃🍸🍹🧉🧂🥢🥡🥣🍽🍴🥄🧊🍾⚽️🏀🏈⚾️🥎🎾🏐🏉🥏🪃🏏🥍🏑🏒🏸🏓🪀🎱🥅⛳️🪁🛝🏹🎣🤿🥊🥋🏂⛷🎿🥌⛸🛷🛼🛹🎽🪂🏋️‍♀️🏋️🏋️‍♂️🤼‍♀️🤼🤼‍♂️🤸‍♀️🤸🏌️‍♀️🤾‍♂️🤾🤾‍♀️🤺⛹️‍♂️⛹️⛹️‍♀️🤸‍♂️🏌️🏌️‍♂️🏇🧘‍♀️🧘🧘‍♂️🏄‍♀️🏄🏄‍♂️🚣‍♂️🚣🚣‍♀️🤽‍♂️🤽🤽‍♀️🏊‍♂️🏊🏊‍♀️🧗‍♀️🧗🧗‍♂️🚵‍♀️🚵🚵‍♂️🚴‍♀️🚴🚴‍♂️🎫🎗🏵🎖🏅🥉🥈🥇🏆🎟🎪🤹‍♀️🤹🤹‍♂️🎭🩰🎨🎬🪗🎺🎷🪘🥁🎹🎼🎧🎤🎸🪕🎻🎲♟🎯🎳🎮🎰🧩🚗🚕🚙🚌🚎🏎🚓🚑🚒🩼🦼🦽🦯🚜🚛🚚🛻🚐🛴🚲🛵🏍🛺🛞🚨🚔🚍🚝🚞🚋🚃🚟🚠🚡🚖🚘🚄🚅🚈🚂🚆🚇🚊🚉✈️🛶🚁🛸🚀🛰💺🛩🛬🛫⛵️🚤🛥🛳🚢🛟⛴⚓️🪝🗼🗽🗿🗺🚏🚥🚦🚧⛽️🏰🏯🏟🎡🎢🎠⛲️⛱🏖🛖⛺️🏕🗻🏔⛰🌋🏜🏝🏠🏡🏘🏚🏗🏭🏢🏬🏣🏛💒🏩🏫🏪🏨🏦🏥🏤⛪️🕌🕍🛕🕋⛩🛤🛣🗾🌆🌇🎆🎇🌠🌄🌅🏞🎑🏙🌃🌌🌉🌁⌚️📱📲💻⌨️🖥🖨🖱🖲📸📷📼📀💿💾💽🗜🕹📹🎥📽🎞📞☎️📟📠📺🕰⏰⏲⏱🧭🎛🎚🎙📻⌛️⏳📡🔋🪫🔌💡🔦🕯🪙💷💶💴💵💸🛢🧯🪔💰💳🪪💎⚖️🪜🧰🪛🔧🧱🪤⚙️🔩🪚⛏🛠⚒🔨⛓🧲🔫💣🧨🪓🔪🗡⚔️🧿📿🔮🏺⚱️🪦⚰️🚬🛡🪬💈⚗️🔭🔬🕳🩻🩹🩺🧹🌡🧪🧫🦠🧬🩸💉💊🪠🧺🧻🚽🚰🚿🛁🛀🧼🚪🗝🔑🛎🧴🪣🧽🪒🪥🪑🛋🛏🛌🧸🪆🖼🪞🪟🎊🪅🪄🎀🎏🎈🎁🛒🛍🎉🎎🏮🎐🪩🧧✉️📩📨📫📪🪧🏷📦📤📥💌📧📬📭📮📯📜📃📄📑🧾📇🗑📅📆🗓🗒📉📈📊🗃🗳🗄📋📂📁🗂🗞📰📖📚📙📘📗📕📒📔📓🔖🧷🔗📎🖇📐📏🧮📌✏️📝🖍🖌✒️🖋🖊✂️📍🔍🔎🔏🔐🔒🔓❤️🧡💛💚💙💜🤎🖤🤍⚪️⚫️🟤🟣🔵🟢🟡🟠🔴🟥🟧🟨🟩🟦🟫⬛️⬜️❤️‍🩹❣️❤️‍🔥💔🏿🏾🏽🏼🏻💕💞💓💗💖💘💝💟☮️☦️☯️🕎🔯✡️☸️🕉☪️✝️🛐⛎♈️♉️♊️♋️♌️♍️♎️⚛️🆔♓️♒️♑️♐️♏️🉑☢️📴📳🈶🈚️🈸🈺🈷️✴️🈲🈹🈵🈴㊗️㊙️🉐💮🆚🅰️🅱️🆎🅾️🆑🆘❌⭕️🛑🚳🚯🚷♨️💢💯🚫📛⛔️🚱🔞📵🚭❗️❕❓❔‼️🔰⚜️🔱🚸⚠️〽️🔆🔅⁉️♻️✅🈯️💹❇️✳️❎🌐💠🈳🛗🅿️♿️🚾🏧💤🌀Ⓜ️🈂️🛂🛃🛄🛅🚹🚺🚼⚧🔡🔤ℹ️🔣🈁📶🎦🚮🚻🔠🆖🆗🆙🆒🆕🆓0️⃣1️⃣🔟9️⃣8️⃣7️⃣6️⃣5️⃣4️⃣3️⃣2️⃣🔢#️⃣*️⃣⏏️▶️⏸⏯⏹⏺🔽🔼◀️⏬⏫⏪⏩⏮⏭➡️⬅️⬆️⬇️↗️↘️↙️↖️↕️🔄🔂🔁🔀⤵️⤴️↩️↪️↔️🔃🎵🎶➕➖➗✖️🟰♾️🔛🔙🔚👁‍🗨®️©️™️💱💲🔝🔜〰️➰➿✔️☑️🔘🔺▫️▪️🔲🔳🔷🔶🔹🔸🔻◾️◽️◼️◻️🔈🔇🔉🔊🔔♥️♣️♠️🗯💭💬📢📣🔕♦️🃏🎴🀄️🕐🕑🕒🕓🕔🕝🕜🕛🕚🕙🕘🕗🕖🕕🕞🕟🕠🕡🕢🕣🕤🕥🕦🕧🏳️🏴🏴‍☠️🏁🚩🏳️‍🌈🏳️‍⚧️🇺🇳🇮🇸🇦🇱🇦🇼🇦🇷🇩🇿🇦🇪🇺🇸🇦🇫🇦🇿🇮🇪🇦🇲🇦🇮🇦🇴🇦🇬🇦🇩🇾🇪🇬🇧🏴󠁧󠁢󠁥󠁮󠁧󠁿🏴󠁧󠁢󠁳󠁣󠁴󠁿🇺🇬🇼🇫🇮🇩🇮🇳🇮🇷🇮🇶🇮🇹🇮🇱🏴󠁧󠁢󠁷󠁬󠁳󠁿🇺🇦🇺🇿🇺🇾🇪🇨🇪🇬🇪🇪🇸🇿🇪🇹🇪🇷🇨🇻🇬🇭🇧🇶🇳🇱🇴🇲🇦🇽🇦🇹🇦🇺🇸🇻🇬🇬🇬🇾🇰🇿🇨🇦🇮🇨🇬🇦🇨🇲🇬🇲🇰🇬🇰🇮🇬🇷🇨🇼🇨🇺🇨🇾🇬🇼🇬🇳🇰🇭🇬🇹🇬🇵🇬🇺🇰🇼🇨🇰🇬🇱🇨🇽🇬🇩🇭🇷🇨🇬🇨🇴🇰🇲🇽🇰🇨🇷🇨🇨🇨🇮🇰🇪🇰🇾🇨🇩🇸🇦🇬🇸🇼🇸🇧🇱🇸🇹🇿🇲🇵🇲🇸🇲🇸🇽🇸🇬🇸🇾🇬🇪🇯🇲🇯🇪🇬🇮🇩🇯🇸🇱🇿🇼🇨🇭🇸🇪🇸🇩🇪🇸🇸🇷🇱🇰🇸🇰🇸🇮🇸🇧🇸🇴🇱🇨🇸🇭🇻🇨🇰🇳🇷🇸🇸🇳🇸🇨🇹🇨🇹🇭🇹🇯🇨🇿🇮🇴🇹🇩🇹🇳🇨🇱🇹🇲🇹🇹🇩🇲🇩🇴🇹🇰🇹🇬🇩🇪🇩🇰🇹🇻🇹🇷🇹🇴🇳🇬🇳🇷🇳🇦🇳🇺🇳🇮🇳🇪🇳🇨🇻🇦🇵🇰🇭🇹🇧🇭🇳🇴🇳🇫🇳🇵🇳🇿🇻🇺🇧🇸🇵🇬🇧🇲🇵🇼🇵🇾🇧🇧🇵🇸🇭🇺🇫🇰🇫🇴🇵🇷🇧🇹🇫🇮🇵🇭🇫🇯🇵🇳🇧🇩🇧🇷🇫🇷🇧🇬🇧🇫🇧🇳🇧🇮🇻🇳🇧🇯🇻🇪🇵🇹🇧🇴🇧🇼🇧🇦🇵🇱🇧🇪🇵🇪🇧🇿🇧🇾🇭🇳🇲🇭🇲🇴🇲🇬🇾🇹🇲🇼🇲🇱🇲🇹🇲🇶🇲🇨🇲🇿🇲🇷🇲🇺🇲🇽🇲🇲🇫🇲🇮🇲🇲🇾🇲🇻🇲🇩🇲🇦🇲🇳🇲🇪🇲🇸🇯🇴🇱🇻🇱🇧🇱🇸🇷🇼🇱🇺🇷🇴🇱🇷🇱🇮🇱🇾🇱🇹🇷🇪🇷🇺🇻🇬🇪🇺🇰🇷🇪🇭🇭🇰🇬🇶🇨🇫🇨🇳🇹🇱🇸🇸🇦🇶🇯🇵🎌🇬🇫🇵🇫🇰🇵🇲🇵🇲🇰🇦🇸🇻🇮🇹🇫✡✳✴❇☯✂☘✉✔✘⚧✏✒♟\" + \\\n",
        "\"]+)\"\n",
        "\n",
        "LABEL_EQUIVALENCE = pd.read_excel(googleSheetURL, sheet_name=\"Fact Check Equivalence\", header=None).to_numpy().tolist()\n",
        "\n",
        "def processPost(str, swapII):\n",
        "  str = removeHebrewAndArabic(str)\n",
        "\n",
        "  if(swapII):\n",
        "    str = swapIdentifyingInfo(str)\n",
        "\n",
        "  return str\n",
        "\n",
        "def removeHebrewAndArabic(str):\n",
        "  str = str.translate( { ord(i): None for i in HEBREW_CHARACTERS } )\n",
        "  str = str.translate( { ord(i): None for i in ARABIC_CHARACTERS } )\n",
        "  return str\n",
        "\n",
        "def swapIdentifyingInfo(str):\n",
        "  splitStr = re.split(SPLIT_DELIMS, str)\n",
        "  for i in range(0, len(splitStr)):\n",
        "    replacement = None\n",
        "    if(splitStr[i].lower() in ISRAEL_IDENTIFIERS):\n",
        "      replacement = PALESTINE_IDENTIFIERS[ISRAEL_IDENTIFIERS.index(splitStr[i].lower())]\n",
        "    elif(splitStr[i].lower() in PALESTINE_IDENTIFIERS):\n",
        "      replacement = ISRAEL_IDENTIFIERS[PALESTINE_IDENTIFIERS.index(splitStr[i].lower())]\n",
        "\n",
        "    if(replacement != None):\n",
        "      if(splitStr[i] == splitStr[i].capitalize()):\n",
        "          splitStr[i] = replacement.capitalize()\n",
        "      elif(splitStr[i].isupper()):\n",
        "          splitStr[i] = replacement.upper()\n",
        "      else:\n",
        "          splitStr[i] = replacement\n",
        "\n",
        "  return \"\".join(splitStr)\n",
        "\n",
        "\n",
        "def standardizeLabel(str):\n",
        "  for i in range(0, len(LABEL_EQUIVALENCE)):\n",
        "    if(str in LABEL_EQUIVALENCE[i]):\n",
        "      return LABEL_EQUIVALENCE[i][0]\n",
        "  assert(\"LABEL NOT FOUND\")\n",
        "  return str.capitalize()\n",
        "\n",
        "PROCESSED_DATA = []\n",
        "\n",
        "for i in range(0, len(IMPORTED_DATA)):\n",
        "  newEntry = [processPost(IMPORTED_DATA[i][0], i % 2), # swap identifying info for every other entry\n",
        "              standardizeLabel(IMPORTED_DATA[i][1])]\n",
        "  PROCESSED_DATA.append(newEntry)\n",
        "\n",
        "print(np.array(PROCESSED_DATA))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Suppressing ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "text_data = [entry[0] for entry in PROCESSED_DATA]\n",
        "labels = [entry[1] for entry in PROCESSED_DATA]\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 'True':\n",
        "    labels[i] = True\n",
        "  if labels[i] == 'False':\n",
        "    labels[i] = False\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), alpha=0.001, learning_rate_init=0.01, max_iter=1000, random_state=42, momentum=0.9, solver='adam')\n",
        "mlp.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "# Evaluate this model\n",
        "predict_test = mlp.predict(X_test)\n",
        "predict_train = mlp.predict(X_train_res)\n",
        "\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, predict_test))\n",
        "print(\"Train Classification Report:\\n\", classification_report(y_train_res, predict_train))\n"
      ],
      "metadata": {
        "id": "U5BMxHkt-Lcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab7c816-bc4c-4a02-f3cd-58043e2f5445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.62      0.71      0.67         7\n",
            "        True       0.33      0.25      0.29         4\n",
            "\n",
            "    accuracy                           0.55        11\n",
            "   macro avg       0.48      0.48      0.48        11\n",
            "weighted avg       0.52      0.55      0.53        11\n",
            "\n",
            "Train Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      1.00      1.00        28\n",
            "        True       1.00      1.00      1.00        28\n",
            "\n",
            "    accuracy                           1.00        56\n",
            "   macro avg       1.00      1.00      1.00        56\n",
            "weighted avg       1.00      1.00      1.00        56\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bayes - Zaid\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "text_data = [entry[0] for entry in PROCESSED_DATA]\n",
        "labels = [entry[1] for entry in PROCESSED_DATA]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "best_alpha = 0\n",
        "best_accuracy = 0\n",
        "best_y_pred = None\n",
        "\n",
        "for alpha in np.arange(0.1, 1.1, 0.1):\n",
        "    model = MultinomialNB(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Alpha: {alpha:.1f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_alpha = alpha\n",
        "        best_y_pred = y_pred\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}, Best Accuracy: {best_accuracy}\")\n",
        "print(\"Confusion Matrix for Best Alpha:\\n\", confusion_matrix(y_test, best_y_pred))\n",
        "print(\"Classification Report for Best Alpha:\\n\", classification_report(y_test, best_y_pred))\n"
      ],
      "metadata": {
        "id": "KSoXa9g2V-4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c73071-2c2c-429b-eb4e-20cdc6e66514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.1, Accuracy: 0.5455\n",
            "Alpha: 0.2, Accuracy: 0.6364\n",
            "Alpha: 0.3, Accuracy: 0.7273\n",
            "Alpha: 0.4, Accuracy: 0.8182\n",
            "Alpha: 0.5, Accuracy: 0.9091\n",
            "Alpha: 0.6, Accuracy: 0.9091\n",
            "Alpha: 0.7, Accuracy: 0.9091\n",
            "Alpha: 0.8, Accuracy: 0.9091\n",
            "Alpha: 0.9, Accuracy: 0.9091\n",
            "Alpha: 1.0, Accuracy: 0.8182\n",
            "Best Alpha: 0.5, Best Accuracy: 0.9090909090909091\n",
            "Confusion Matrix for Best Alpha:\n",
            " [[9 0]\n",
            " [1 1]]\n",
            "Classification Report for Best Alpha:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.90      1.00      0.95         9\n",
            "        True       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.91        11\n",
            "   macro avg       0.95      0.75      0.81        11\n",
            "weighted avg       0.92      0.91      0.90        11\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM - Ahyush\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "data = np.array(PROCESSED_DATA)\n",
        "X = data[:, 0]\n",
        "y = data[:, 1]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.25, random_state=42)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [1, 0.1, 0.01],\n",
        "    'kernel': ['rbf', 'linear','poly']\n",
        "}\n",
        "svm = SVC(class_weight='balanced')\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "\n",
        "\n",
        "print(y_train_smote)\n",
        "\n",
        "print(\"-------------------------------\")\n",
        "\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "U75x2cwDM0Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b6132b-b3c3-48d1-86d1-6c6fe96a2707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.75      0.90      0.82        10\n",
            "        True       0.50      0.25      0.33         4\n",
            "\n",
            "    accuracy                           0.71        14\n",
            "   macro avg       0.62      0.57      0.58        14\n",
            "weighted avg       0.68      0.71      0.68        14\n",
            "\n",
            "Best Parameters: {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
            "['False' 'False' 'False' 'True' 'True' 'False' 'True' 'True' 'False'\n",
            " 'True' 'False' 'False' 'False' 'False' 'False' 'True' 'True' 'True'\n",
            " 'True' 'False' 'False' 'False' 'False' 'False' 'False' 'True' 'False'\n",
            " 'False' 'False' 'True' 'True' 'False' 'True' 'True' 'False' 'False'\n",
            " 'False' 'True' 'False' 'False' 'True' 'True' 'True' 'True' 'True' 'True'\n",
            " 'True' 'True' 'True' 'True']\n",
            "-------------------------------\n",
            "['False' 'False' 'False' 'False' 'False' 'True' 'False' 'False' 'True'\n",
            " 'False' 'False' 'False' 'True' 'True']\n"
          ]
        }
      ]
    }
  ]
}
